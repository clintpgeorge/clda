% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{clda_ags_em}
\alias{clda_ags_em}
\title{cLDA: Auxiliary Variable Update within Collpased Gibbs Sampler with
hyperparameter \eqn{\alpha} sampling  and EM updates for hyperparameters
\eqn{\eta} and \eqn{\gamma}}
\usage{
clda_ags_em(num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h,
  em_max_iter, gibbs_max_iter, burn_in, spacing, save_pi, save_theta, save_beta,
  save_lp, verbose, init_pi, test_doc_share = 0, test_word_share = 0,
  burn_in_pi = 10L, sample_alpha_h = FALSE, gamma_shape = 1,
  gamma_rate = 1)
}
\arguments{
\item{num_topics}{Number of topics in the corpus}

\item{vocab_size}{Vocabulary size}

\item{docs_cid}{Collection ID for each document in the corpus (indices starts 0)}

\item{docs_tf}{Corpus documents read from the Blei corpus format, e.g., via \code{\link{read_docs}} (indices starts with 0)}

\item{alpha_h}{Hyperparameter for \eqn{\pi}. When \code{sample_alpha_h} is \code{true} this variable is used to initialize hyperparameter \eqn{\alpha}}

\item{gamma_h}{Hyperparameter for \eqn{\theta}}

\item{eta_h}{Hyperparameter for \eqn{\beta}}

\item{em_max_iter}{Maximum number of EM iterations to be performed}

\item{gibbs_max_iter}{Maximum number of Gibbs iterations to be performed}

\item{burn_in}{Burn-in-period for the Gibbs sampler}

\item{spacing}{Spacing between the stored samples (to reduce correlation)}

\item{save_pi}{if 0 the function does not save \eqn{\pi} samples}

\item{save_theta}{if 0 the function does not save \eqn{\theta} samples}

\item{save_beta}{if 0 the function does not save \eqn{\beta} samples}

\item{save_lp}{if 0 the function does not save computed log posterior for iterations}

\item{verbose}{from {0, 1, 2}}

\item{init_pi}{the initial configuration for the collection level topic mixtures, i.e., \eqn{\pi} samples}

\item{test_doc_share}{proportion of the test documents in the corpus. Must be from [0., 1.)}

\item{test_word_share}{proportion of the test words in each test document. Must be from [0., 1.)}

\item{burn_in_pi}{burn in iterations until pi sampling}

\item{sample_alpha_h}{sample hyperparameter \eqn{\alpha} (true) or not (false)}

\item{gamma_shape}{hyperparameter \code{shape} for the Gamma prior on \eqn{\alpha}. Default is 1.}

\item{gamma_rate}{hyperparameter \code{rate} for the Gamma prior on \eqn{\alpha}. Default is 1.}
}
\value{
A list of
  \item{corpus_topic_counts}{corpus-level topic counts from last iteration of the Markov chain}
  \item{pi_counts}{collection-level topic counts from the last iteration of the Markov chain}
  \item{theta_counts}{document-level topic counts from last iteration of the Markov chain}
  \item{beta_counts}{topic word counts from last iteration of the Markov chain}
  \item{pi_samples}{\eqn{\pi} samples after the burn in period, if \code{save_pi} is set}
  \item{theta_samples}{\eqn{\theta} samples after the burn in period, if \code{save_theta} is set}
  \item{beta_samples}{\eqn{\beta} samples after the burn in period, if \code{save_beta} is set}
  \item{log_posterior}{the log posterior (upto a constant multiplier) of the hidden variable \eqn{\psi = (\beta, \pi, \theta, z)} in the LDA model, if \code{save_lp} is set}
  \item{log_posterior_pi_z}{the log posterior (upto a constant multiplier) of the hidden variables \eqn{(\pi, z)} in the LDA model, if \code{save_lp} is set}
  \item{perplexity}{perplexity of the set of held-out words}
  \item{alpha_h_samples}{\eqn{\alpha} samples if \code{sample_alpha_h} is \code{true}}
  \item{gamma_h_estimates}{\eqn{\gamma} estimates from each EM iteration}
  \item{eta_h_estimates}{\eqn{\eta} estimates from each EM iteration}
}
\description{
This implements a Markov chain on \eqn{(z, \pi)} via the collapsed Gibbs
sampling with auxiliary variable updates for the compound latent Dirichlet
allocation (cLDA) model.
}
\details{
To compute perplexity, we first partition words in a corpus into two sets:
(a) a test set (held-out set), which is selected from the set of words in
the test (held-out) documents (identified via \code{test_doc_share} and
\code{test_word_share}) and (b) a training set, i.e., the remaining words in
the corpus. We then run the variational EM algorithm based on the training
set. Finally, we compute per-word perplexity based on the held-out set.
}
\note{
Updated on: December 18, 2017 -- Added hyperparameter alpha sampling and AGS EM updates

Updated on: June 02, 2016

Created on: May 18, 2016

Created by: Clint P. George
}
\seealso{
Other MCMC: \code{\link{clda_ags_sample_alpha}},
  \code{\link{clda_ags}}, \code{\link{clda_mgs}},
  \code{\link{lda_cgs}}
}
