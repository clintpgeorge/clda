# This file was generated by Rcpp::compileAttributes
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' cLDA: Auxiliary Variable Update within Collpased Gibbs Sampler
#'
#' This implements a Markov chain on \eqn{(z, \pi)} via the collapsed Gibbs
#' sampling with auxiliary variable updates for the compound latent Dirichlet
#' allocation (cLDA) model.
#'
#' To compute perplexity, we first partition words in a corpus into two sets:
#' (a) a test set (held-out set), which is selected from the set of words in
#' the test (held-out) documents (identified via \code{test_doc_share} and
#' \code{test_word_share}) and (b) a training set, i.e., the remaining words in
#' the corpus. We then run the variational EM algorithm based on the training
#' set. Finally, we compute per-word perplexity based on the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_cid Collection ID for each document in the corpus (indices starts 0)
#' @param docs_tf Corpus documents read from the Blei corpus format, e.g., via \code{\link{read_docs}} (indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\pi}
#' @param gamma_h Hyperparameter for \eqn{\theta}
#' @param eta_h Hyperparameter for \eqn{\beta}
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_pi if 0 the function does not save \eqn{\pi} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for iterations
#' @param verbose from {0, 1, 2}
#' @param init_pi the initial configuration for the collection level topic mixtures, i.e., \eqn{\pi} samples
#' @param test_doc_share proportion of the test documents in the corpus.
#'   Must be from [0., 1.)
#' @param test_word_share proportion of the test words in each test document.
#'   Must be from [0., 1.)
#'
#' @return A list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{pi_counts}{collection-level topic counts from the last iteration of
#'   the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov
#'   chain}
#'   \item{pi_samples}{\eqn{\pi} samples after the burn in period, if
#'   \code{save_pi} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \pi, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{log_posterior_pi_z}{the log posterior (upto a constant multiplier)
#'   of the hidden variables \eqn{(\pi, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#'
#' @export
#'
#' @family MCMC
#'
#' @note
#'
#' Modified on: June 02, 2016
#'
#' Created on: May 18, 2016
#'
#' Created by: Clint P. George
#'
clda_ags <- function(num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, max_iter, burn_in, spacing, save_pi, save_theta, save_beta, save_lp, verbose, init_pi, test_doc_share = 0., test_word_share = 0., burn_in_pi = 10L) {
    .Call('clda_clda_ags', PACKAGE = 'clda', num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, max_iter, burn_in, spacing, save_pi, save_theta, save_beta, save_lp, verbose, init_pi, test_doc_share, test_word_share, burn_in_pi)
}

#' CLDA: MMALA within Gibbs sampler (with re-parameterization)
#'
#' This implements a Markov chain on \eqn{(z, \pi)} via the Metropolis adjusted
#' Langevin algorithm within Gibbs sampler (MGS) for the compound latent
#' Dirichlet allocation (cLDA) model.
#'
#' To compute perplexity, we first partition words in a corpus into two sets:
#' (a) a test set (held-out set), which is selected from the set of words in
#' the test (held-out) documents (identified via \code{test_doc_share} and
#' \code{test_word_share}) and (b) a training set, i.e., the remaining words in
#' the corpus. We then run the variational EM algorithm based on the training
#' set. Finally, we compute per-word perplexity based on the held-out set.
#'
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_cid Documents collection IDs (ID indices starts 0)
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\pi}
#' @param gamma_h Hyperparameter for \eqn{\theta}
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param step_size Step size for Langevin update
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_pi if 0 the function does not save \eqn{\pi} samples
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param init_pi the initial configuration of the collection level topic
#'                mixtures, i.e., \eqn{\pi} samples
#' @param test_doc_share proportion of the test documents in the corpus.
#'   Must be from [0., 1.)
#' @param test_word_share proportion of the test words in each test document.
#'   Must be from [0., 1.)
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{pi_counts}{collection-level topic counts from the last iteration of
#'   the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov
#'   chain}
#'   \item{pi_samples}{\eqn{\pi} samples after the burn in period, if
#'   \code{save_pi} is set}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \pi, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{log_posterior_pi_z}{the log posterior (upto a constant multiplier)
#'   of the hidden variables \eqn{(\pi, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#'
#' @export
#'
#' @family MCMC
#'
#' @note A Leapling
#'
#' Modified on: May 18, 2016
#'
#' Created on: Februray 29, 2016
#'
#' Created by: Clint P. George
#'
clda_mgs <- function(num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, step_size, max_iter, burn_in, spacing, save_pi, save_theta, save_beta, save_lp, verbose, init_pi, test_doc_share = 0., test_word_share = 0., burn_in_pi = 10L) {
    .Call('clda_clda_mgs', PACKAGE = 'clda', num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, step_size, max_iter, burn_in, spacing, save_pi, save_theta, save_beta, save_lp, verbose, init_pi, test_doc_share, test_word_share, burn_in_pi)
}

#' cLDA: Variational Expectation Maximization
#'
#' This implements the Variational Expectation Maximization (EM) algorithm for
#' the compound latent Dirichlet allocation (cLDA) model.
#'
#' To compute perplexity, we first partition words in a corpus into two sets:
#' (a) a test set (held-out set), which is selected from the set of words in
#' the test (held-out) documents (identified via \code{test_doc_share} and
#' \code{test_word_share}) and (b) a training set, i.e., the remaining words in
#' the corpus. We then run the variational EM algorithm based on the training
#' set. Finally, we compute per-word perplexity based on the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size Vocabulary size
#' @param docs_cid Documents collection IDs (ID indices starts 0)
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for collection-level Dirichlets \eqn{\pi}
#' @param gamma_h Hyperparameter for document-level Dirichlets \eqn{\theta}
#' @param eta_h Hyperparameter for corpus level topic Dirichlets \eqn{\beta}
#' @param vi_max_iter Maximum number of iterations for variational inference
#' @param em_max_iter Maximum number of iterations for variational EM
#' @param vi_conv_thresh Convergence threshold for the document variational inference loop
#' @param em_conv_thresh Convergence threshold for the variational EM loop
#' @param tau_max_iter Maximum number of iterations for the constraint Newton updates of \eqn{\tau}
#' @param tau_step_size the step size for the constraint Newton updates of \eqn{\tau}
#' @param estimate_alpha If true, run hyperparameter \eqn{\alpha} optimization
#' @param estimate_gamma dummy parameter [not implemented]
#' @param estimate_eta If true, run hyperparameter \eqn{\eta} optimization
#' @param verbose from {0, 1, 2, 3}
#' @param init_pi the initial configuration for the collection level topic
#'                mixtures, i.e., \eqn{\pi} samples
#' @param test_doc_share proportion of the test documents in the corpus.
#'   Must be from [0., 1.)
#' @param test_word_share proportion of the test words in each test document.
#'   Must be from [0., 1.)
#'
#' @return A list of variational parameters
#'
#' @export
#'
#' @family Variational Inference
#'
#' @note Created on May 13, 2016
#'
clda_vem <- function(num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, vi_max_iter, em_max_iter, vi_conv_thresh, em_conv_thresh, tau_max_iter, tau_step_size, estimate_alpha, estimate_gamma, estimate_eta, verbose, init_pi, test_doc_share = 0., test_word_share = 0.) {
    .Call('clda_clda_vem', PACKAGE = 'clda', num_topics, vocab_size, docs_cid, docs_tf, alpha_h, gamma_h, eta_h, vi_max_iter, em_max_iter, vi_conv_thresh, em_conv_thresh, tau_max_iter, tau_step_size, estimate_alpha, estimate_gamma, estimate_eta, verbose, init_pi, test_doc_share, test_word_share)
}

#' LDA: Collapsed Gibbs Sampler with Perplexity Computation
#'
#' This implements of the collapsed Gibbs sampler for the LDA model---a Markov
#' chain on \eqn{z}.
#'
#' To compute perplexity, we first partition words in a corpus into two sets:
#' (a) a test set (held-out set), which is selected from the set of words in
#' the test (held-out) documents (identified via \code{test_doc_share} and
#' \code{test_word_share}) and (b) a training set, i.e., the remaining words in
#' the corpus. We then run the variational EM algorithm based on the training
#' set. Finally, we compute per-word perplexity based on the held-out set.
#'
#' @param num_topics Number of topics in the corpus
#' @param vocab_size  Vocabulary size
#' @param docs_tf A list of corpus documents read from the Blei corpus using
#'             \code{\link{read_docs}} (term indices starts with 0)
#' @param alpha_h Hyperparameter for \eqn{\theta} sampling
#' @param eta_h Smoothing parameter for the \eqn{\beta} matrix
#' @param max_iter Maximum number of Gibbs iterations to be performed
#' @param burn_in Burn-in-period for the Gibbs sampler
#' @param spacing Spacing between the stored samples (to reduce correlation)
#' @param save_beta if 0 the function does not save \eqn{\beta} samples
#' @param save_theta if 0 the function does not save \eqn{\theta} samples
#' @param save_lp if 0 the function does not save computed log posterior for
#'                iterations
#' @param verbose from {0, 1, 2}
#' @param test_doc_share proportion of the test documents in the corpus.
#'   Must be from [0., 1.)
#' @param test_word_share proportion of the test words in each test document.
#'   Must be from [0., 1.)
#'
#' @return The Markov chain output as a list of
#'   \item{corpus_topic_counts}{corpus-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{theta_counts}{document-level topic counts from last iteration
#'   of the Markov chain}
#'   \item{beta_counts}{topic word counts from last iteration of the Markov chain}
#'   \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
#'   \code{save_theta} is set}
#'   \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
#'   \code{save_beta} is set}
#'   \item{log_posterior}{the log posterior (upto a constant multiplier) of
#'   the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
#'   if \code{save_lp} is set}
#'   \item{perplexity}{perplexity of the held-out words' set}
#'
#' @export
#'
#' @family MCMC
#'
lda_cgs <- function(num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_theta, save_beta, save_lp, verbose, test_doc_share = 0., test_word_share = 0.) {
    .Call('clda_lda_cgs', PACKAGE = 'clda', num_topics, vocab_size, docs_tf, alpha_h, eta_h, max_iter, burn_in, spacing, save_theta, save_beta, save_lp, verbose, test_doc_share, test_word_share)
}

#' Samples from the Antoniak distribution
#'
#' It's done by sampling \eqn{N} Bernoulli variables
#'
#' References:
#'
#'   http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf
#'
#' @param N Number of samples
#' @param alpha strength parameter
#'
#' @export
#'
#' @family utils
#'
#' @note
#'
#' Created on: May 19, 2016
#'
#' Created by: Clint P. George
#'
sample_antoniak <- function(N, alpha) {
    .Call('clda_sample_antoniak', PACKAGE = 'clda', N, alpha)
}

#' A speedy sampling from a multimomial distribution
#'
#' @param theta a multinomial probability vector (K x 1 vector)
#'
#' @return returns a class index from [0, K)
#'
#' @note
#' Author: Clint P. George
#'
#' Created on: February 11, 2016
#'
#' @family utils
#'
#' @export
sample_multinomial <- function(theta) {
    .Call('clda_sample_multinomial', PACKAGE = 'clda', theta)
}

#' Samples from a Dirichlet distribution given a hyperparameter
#'
#' @param num_elements the dimention of the Dirichlet distribution
#' @param alpha the hyperparameter vector (a column vector)
#'
#' @return returns a Dirichlet sample (a column vector)
#'
#' @note
#' Author: Clint P. George
#'
#' Created on: 2014
#'
#' @family utils
#'
#' @export
sample_dirichlet <- function(num_elements, alpha) {
    .Call('clda_sample_dirichlet', PACKAGE = 'clda', num_elements, alpha)
}

